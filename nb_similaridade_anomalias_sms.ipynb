{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0988c328-6a41-4568-982e-d24cd915bba3",
     "showTitle": true,
     "title": "Parâmetros do Algoritmo"
    }
   },
   "outputs": [],
   "source": [
    "# Widgets para obtenção de parametros para execução do algoritmo\n",
    "\n",
    "dbutils.widgets.text(\"repartition\", \"100\")\n",
    "repartition_param = dbutils.widgets.get(\"repartition\")\n",
    "\n",
    "dbutils.widgets.text(\"aplicacao\", \"similaridade_anomalia\")\n",
    "aplicacao = dbutils.widgets.get(\"aplicacao\")\n",
    "\n",
    "dbutils.widgets.text(\"tabela_controle\", \"controle_similaridade\")\n",
    "tabela_controle = dbutils.widgets.get(\"tabela_controle\")\n",
    "\n",
    "dbutils.widgets.text(\"data_anomalia_inicio\", \"2000-01-01\")\n",
    "data_anomalia_inicio_param = dbutils.widgets.get(\"data_anomalia_inicio\")\n",
    "\n",
    "dbutils.widgets.text(\"storage_account\", \"storage_account\")\n",
    "storage_account_param = dbutils.widgets.get(\"storage_account\")\n",
    "\n",
    "dbutils.widgets.text(\"container\", \"pouso\")\n",
    "container_param = dbutils.widgets.get(\"container\")\n",
    "\n",
    "dbutils.widgets.text(\"input_path_anomalia\", \"input/anomalia\")\n",
    "input_path_anomalia = dbutils.widgets.get(\"input_path_anomalia\")\n",
    "\n",
    "dbutils.widgets.text(\"input_path_resultado\", \"input/relacao_anomalia\")\n",
    "input_path_resultado = dbutils.widgets.get(\"input_path_resultado\")\n",
    "\n",
    "dbutils.widgets.text(\"output_path\", \"output/relacao_anomalia\")\n",
    "output_path_param = dbutils.widgets.get(\"output_path\")\n",
    "\n",
    "print(\"repartition\", repartition_param)\n",
    "print(\"aplicacao\", aplicacao)\n",
    "print(\"tabela_controle\", tabela_controle)\n",
    "print(\"data_anomalia_inicio\", data_anomalia_inicio_param)\n",
    "print(\"storage_account_param\", storage_account_param)\n",
    "print(\"container_param\", container_param)\n",
    "print(\"input_path_anomalia\", input_path_anomalia)\n",
    "print(\"input_path_resultado\", input_path_resultado)\n",
    "print(\"output_path_param\", output_path_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e1f290fc-5323-4521-8cdf-311fbeaadc4e",
     "showTitle": true,
     "title": "Obtem data da ultima analise"
    }
   },
   "outputs": [],
   "source": [
    "controle_df = spark.sql(\"select * from {aplicacao}.{tabela_controle}\".format(aplicacao=aplicacao, tabela_controle=tabela_controle))\n",
    "data = controle_df.select(\"last_date\").collect()\n",
    "data_ultima_avaliacao_param = data[0][0]\n",
    "print(\"data_ultima_avaliacao_param\", data_ultima_avaliacao_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "10b019f5-a34a-45f3-a6a6-2548c8628a9b",
     "showTitle": true,
     "title": "Instalações"
    }
   },
   "outputs": [],
   "source": [
    "!python -m spacy download pt_core_news_sm\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3e768a04-ceed-441b-ae25-e49cf7879f40",
     "showTitle": true,
     "title": "Funções utilitárias FS"
    }
   },
   "outputs": [],
   "source": [
    "def create_mount(nome_container, path_mountpoint, storage_account, application_id, directory_id, service_credential):\n",
    "        \n",
    "  path = \"/mnt/\" + path_mountpoint\n",
    "  \n",
    "  SOURCE = \"abfss://{container}@{storage_acct}.dfs.core.windows.net/\".format(container=nome_container, storage_acct=storage_account)\n",
    "  CONFIGS = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "           \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "           \"fs.azure.account.oauth2.client.id\": application_id,\n",
    "           \"fs.azure.account.oauth2.client.secret\": service_credential,\n",
    "           \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/{directory_id}/oauth2/token\".format(directory_id=directory_id)}        \n",
    "    \n",
    "  try:\n",
    "\n",
    "      dbutils.fs.mount(\n",
    "          source=SOURCE,\n",
    "          mount_point=path,\n",
    "          extra_configs=CONFIGS)\n",
    "        \n",
    "      print(SOURCE)                \n",
    "  except Exception as e:\n",
    "    \n",
    "    print (str(e))\n",
    "    if \"Directory already mounted\" in str(e):\n",
    "      pass # Ignore error if already mounted.\n",
    "  \n",
    "  print(\"Success.\")\n",
    "    \n",
    "  return path    \n",
    "\n",
    "def unmount(path):\n",
    "  mnts = dbutils.fs.ls(path)\n",
    "  for mnt in mnts: \n",
    "    try:\n",
    "      dbutils.fs.unmount(mnt[0])\n",
    "    except Exception as e:\n",
    "      if \"Directory not mounted\" in str(e):\n",
    "        print(mnt[0] , \" - Não estava montado !\")\n",
    "        pass\n",
    "      else:\n",
    "        print(e) \n",
    "\n",
    "  dbutils.fs.refreshMounts()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eb3325ab-ceec-489f-a7d5-7902a7d03990",
     "showTitle": true,
     "title": "Funções para leitura/escrita de dados"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(path, format=\"parquet\"):\n",
    "  data_df = spark.read.option(\"recursiveFileLookup\",\"true\").format(format).load(path)\n",
    "  return data_df\n",
    "\n",
    "def save_df(df_output, file_dest_path, mode=\"append\", format=\"parquet\"):\n",
    "  df_output.write.mode(mode).format(format).option(\"mergeSchema\", \"true\").save(file_dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7c9283f8-dc03-474f-8d77-11ea14c0011b",
     "showTitle": true,
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType, FloatType, DateType\n",
    "import gensim.parsing.preprocessing as gsp\n",
    "from gensim import utils\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, Normalizer\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "764f3219-7f82-4cc4-b91f-4240f811319b",
     "showTitle": true,
     "title": "Funções de pré-processamento"
    }
   },
   "outputs": [],
   "source": [
    "def lematize(x):\n",
    "    return ' '.join(w.lemma_ for w in nlp(x) if w.pos_ in (\"VERB\", \"NOUN\", \"ADJ\"))\n",
    "\n",
    "def strip_accent(x):\n",
    "    return utils.deaccent(x)\n",
    "    \n",
    "def strip_stopwords(x):\n",
    "    return ' '.join(w for w in x.split() if not w in stops)\n",
    "    \n",
    "filters = [\n",
    "           utils.to_unicode,\n",
    "           gsp.strip_tags, \n",
    "           lematize,\n",
    "           gsp.strip_punctuation,\n",
    "           gsp.strip_multiple_whitespaces,\n",
    "           gsp.strip_numeric,\n",
    "           gsp.strip_short,\n",
    "           strip_stopwords,\n",
    "           strip_accent\n",
    "          ]\n",
    "\n",
    "def clean_text(x):\n",
    "    #Padronizacao em minuscula\n",
    "    s = x.lower()\n",
    "\n",
    "    #Aplicacao dos filtros definidos\n",
    "    for f in filters:\n",
    "        s = f(s)\n",
    "        \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "166562dc-58e9-4841-b05f-60a2c4127c41",
     "showTitle": true,
     "title": "Definição de UDFs"
    }
   },
   "outputs": [],
   "source": [
    "#Funcao udf da limpeza do texto\n",
    "clean_text_udf = F.udf(clean_text, StringType())\n",
    "\n",
    "#Funcao udf do coseno\n",
    "dot_udf = F.udf(lambda x,y: float(x.dot(y)), FloatType()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c2ad8721-9d5f-4bf6-a672-d0492bfdc14d",
     "showTitle": true,
     "title": "Função de avaliação de similaridade"
    }
   },
   "outputs": [],
   "source": [
    "def avalia_similaridade(registrosAnomalia, registrosResultado): \n",
    "  registrosAnomalia = registrosAnomalia.repartition(NR_REPARTITION)\n",
    "\n",
    "  #Remocao de registros com dados obrigatorios nulos\n",
    "  registrosAnomalia = registrosAnomalia.filter(\"id is not null and descricao is not null\")\n",
    "\n",
    "  #Limpeza de dados\n",
    "  registrosAnomaliaCleaned = registrosAnomalia.withColumn(\"descricao\", clean_text_udf(\"descricao\"))\n",
    "  \n",
    "  #Remove registros cuja descricao ficou vazia pos pre processamento\n",
    "  registrosAnomaliaCleaned = registrosAnomaliaCleaned.filter(\"descricao is not null\")\n",
    "  registrosAnomaliaCleaned = registrosAnomaliaCleaned.select(F.col(\"id\"), F.col(\"descricao\"), F.col(\"data_ocorrencia\"), F.col(\"data_inclusao\"))\n",
    "  registrosAnomaliaCleaned.cache()\n",
    "\n",
    "  if registrosAnomaliaCleaned.select('id').head() is None:\n",
    "    print(\"Nao existem registros para montar grupo de anomalias de referencia.\")\n",
    "  else:\n",
    "      #Criacao de pipeline de processamento do texto\n",
    "      tokenizer = Tokenizer(inputCol=\"descricao\", outputCol=\"tokens\")\n",
    "      hashingTF = HashingTF(inputCol=\"tokens\", outputCol=\"rawFeatures\", numFeatures=150)\n",
    "      idfModel = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "      normaliser = Normalizer(inputCol=\"features\", outputCol=\"norm_vec\")\n",
    "      idfPipeline = Pipeline(stages=[tokenizer,hashingTF, idfModel, normaliser])\n",
    "      idfModel = idfPipeline.fit(registrosAnomaliaCleaned)\n",
    "      #Obtencao das features com base no pipeline criado e calculo da norma\n",
    "      normalizedRegistrosAnomalia = idfModel.transform(registrosAnomaliaCleaned)\n",
    "\n",
    "      #Obtem campos relevantes para analise do coseno\n",
    "      df_word_norm_ref = normalizedRegistrosAnomalia.select(normalizedRegistrosAnomalia.id, normalizedRegistrosAnomalia.norm_vec, normalizedRegistrosAnomalia.data_ocorrencia, normalizedRegistrosAnomalia.data_inclusao).cache()\n",
    "\n",
    "      #Selecao das anomalias que serao avaliadas quanto a similaridade\n",
    "      df_word_norm_em_analise = df_word_norm_ref.filter(F.col(\"data_inclusao\") > data_ultima_avaliacao_param)\n",
    "      df_word_norm_em_analise = df_word_norm_em_analise.select(F.col(\"id\").alias(\"id2\"), F.col(\"norm_vec\").alias(\"norm_vec2\"), F.col(\"data_ocorrencia\").alias(\"data_ocorrencia2\")).cache()\n",
    "      \n",
    "      #Remove do escopo \"em analise\" anomalias que já foram analisadas\n",
    "      registrosResultadoDistintos = registrosResultado.select(\"ANAN_CD_ANOMALIA\").distinct()\n",
    "      df_word_norm_em_analise = df_word_norm_em_analise.join(registrosResultadoDistintos, df_word_norm_em_analise.id2 == registrosResultadoDistintos.ANAN_CD_ANOMALIA, how=\"anti\")\n",
    "\n",
    "      #Geracao do produto cartesiano das anomalias para calculo do coseno\n",
    "      cross = df_word_norm_em_analise.crossJoin(df_word_norm_ref)\n",
    "      #Remocao dos registros duplicados\n",
    "      cross = cross.dropDuplicates(['id', 'id2'])\n",
    "      #Remocao das linhas que compara a anomalia consigo mesmo e de anomalias cuja a similar tenha ocorrido apos a em analise\n",
    "      cross = cross.filter(\"id != id2 and data_ocorrencia2 >= data_ocorrencia\")\n",
    "\n",
    "      #Calculo do coseno\n",
    "      cosine = cross.withColumn(\"similarity\", dot_udf(cross.norm_vec, cross.norm_vec2))\n",
    "\n",
    "      #Persistencia do data lake\n",
    "      resultadoSchema = StructType().add(\"anan_cd_anomalia\", StringType()).add(\"anre_cd_anomalia\", StringType()).add(\"selo_cd_selo\", StringType()).add(\"rean_nr_similaridade\", FloatType()).add(\"rean_dt_criacao\", DateType())\n",
    "      resultado = cosine.select(F.col(\"id2\").alias(\"anan_cd_anomalia\"), F.col(\"id\").alias(\"anre_cd_anomalia\"), F.round(cosine[\"similarity\"], 2).alias('rean_nr_similaridade'))\n",
    "      \n",
    "      data_analise = datetime.today()\n",
    "      resultado = resultado.withColumn(\"rean_dt_criacao\", F.lit(data_analise))\n",
    "      \n",
    "      resultado = resultado.withColumn(\"selo_cd_selo\", F.lit(RECORRENTE))\n",
    "\n",
    "      resultado = resultado.select(resultado.anan_cd_anomalia, resultado.anre_cd_anomalia, resultado.selo_cd_selo, resultado.rean_nr_similaridade, resultado.rean_dt_criacao)\n",
    "      resultado = spark.createDataFrame(resultado.rdd, resultadoSchema)\n",
    "      return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f4012aa5-f9a3-422f-a9d1-66dff6b7518b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def presiste_resultado(resultado, output_path, limiar_cosseno):\n",
    "  resultado = resultado.filter(F.col(\"rean_nr_similaridade\") >= limiar)\n",
    "  w = Window.partitionBy(['anan_cd_anomalia', 'anre_cd_anomalia'])\n",
    "  resultado = resultado.withColumn('max_rean_nr_similaridade', F.max('rean_nr_similaridade').over(w))\\\n",
    "    .where(F.col('rean_nr_similaridade') == F.col('max_rean_nr_similaridade'))\\\n",
    "    .drop('max_rean_nr_similaridade')\n",
    "  save_df(resultado, output_path, format=\"delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e71883cf-48fc-48ac-a057-49eadb9b4162",
     "showTitle": true,
     "title": "Setup"
    }
   },
   "outputs": [],
   "source": [
    "#Set configs\n",
    "INPUT_PATH_ANOMALIA = \"/mnt/{container}/{input_path}\".format(container=container_param, input_path=input_path_anomalia)\n",
    "INPUT_PATH_RESULTADO = \"/mnt/{container}/{input_path}\".format(container=container_param, input_path=input_path_resultado)\n",
    "OUTPUT_PATH = \"/mnt/{container}/{output_path}\".format(container=container_param, output_path=output_path_param)\n",
    "STORAGE_ACCOUNT = storage_account_param\n",
    "APPLICATION_ID = \"APPLICATION_ID\"\n",
    "DIRECTORY_ID = \"DIRECTORY_ID\"\n",
    "SERVICE_CREDENTIAL = \"SERVICE_CREDENTIAL\"\n",
    "\n",
    "# Monta volumes de input/output\n",
    "MOUNT = create_mount(container_param, container_param, STORAGE_ACCOUNT, APPLICATION_ID, DIRECTORY_ID, SERVICE_CREDENTIAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bf46e0cc-7ef5-41f4-9a85-40f312acc4a2",
     "showTitle": true,
     "title": "Fluxo de execução"
    }
   },
   "outputs": [],
   "source": [
    "#########Constantes#########\n",
    "#Selo de recorrencia\n",
    "RECORRENTE = 'R'\n",
    "LIMIAR_COSSENO = 0.65\n",
    "\n",
    "#Numero particao\n",
    "NR_REPARTITION = int(repartition_param)\n",
    "\n",
    "#Obtem a lista de stopwords em portugues\n",
    "stops = stopwords.words(\"portuguese\")\n",
    "#Inclusao de stopwords do dominio\n",
    "stops.extend([\"ambos\", \"existente\", \"obs\", \"imediatamente\", \"existente\", \"então\", \"cujo\", \"total\", \"consequencia\", \"consequência\",\"fato\", \"profissional\", \"normalmente\", \"tornar\", \"matrícula\", \"matricula\", \"mat\", \"apenas\", \"pleno\", \"efetuar\", \"possivel\", \"possível\", \"consequente\", \"junior\", \"descricao\", \"descrição\", \"situacao\", \"situação\", \"caracerizar\", \"nome\", \"cada\"])\n",
    "stops = set(stops)\n",
    "\n",
    "#Define o idioma em portugues para realizar a lematizacao\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "#Leitura dos dados a partir do data lake\n",
    "registrosAnomalia = get_data(INPUT_PATH_ANOMALIA, format=\"parquet\")\n",
    "registrosResultado = get_data(INPUT_PATH_RESULTADO, format=\"parquet\")\n",
    "\n",
    "#Faz conversão de tipos e remove datas fora do período de interesse\n",
    "registrosAnomalia = registrosAnomalia.withColumn(\"ANCA_DT_ANOMALIA\", F.to_timestamp(F.col(\"ANCA_DT_ANOMALIA\"), 'yyyy-MM-dd'))\n",
    "registrosAnomalia = registrosAnomalia.withColumn(\"ANCA_DT_INCLUSAO\", F.to_timestamp(F.col(\"ANCA_DT_INCLUSAO\"), 'yyyy-MM-dd HH:mm:ss'))\n",
    "registrosAnomalia = registrosAnomalia.withColumn(\"ANCA_DT_CARGA\", F.to_timestamp(F.col(\"ANCA_DT_CARGA\"), 'yyyy-MM-dd HH:mm:ss'))\n",
    "registrosAnomalia = registrosAnomalia.filter((F.col(\"ANCA_DT_ANOMALIA\") >= data_anomalia_inicio_param) & (F.col(\"ANCA_DT_ANOMALIA\") <= F.current_timestamp()))\n",
    "registrosAnomalia = registrosAnomalia.select(\\\n",
    "                                             F.col(\"ANCA_CD_ANOMALIA\").alias(\"id\"), \\\n",
    "                                             F.col(\"ANCA_DS_ANOMALIA\").alias(\"descricao\"), \\\n",
    "                                             F.col(\"ANCA_TIPO_ACIDENTE\").alias(\"tipo_acidente\"), \\\n",
    "                                             F.col(\"ANCA_NM_TIPO\").alias(\"tipo_anomalia\"),\\\n",
    "                                             F.col(\"ANCA_DT_ANOMALIA\").alias(\"data_ocorrencia\"), \\\n",
    "                                             F.col(\"ANCA_DT_INCLUSAO\").alias(\"data_inclusao\")).distinct()\n",
    "resultado = avalia_similaridade(registrosAnomalia, registrosResultado)\n",
    "presiste_resultado(resultado, OUTPUT_PATH, LIMIAR_COSSENO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7e221450-57df-4248-bd32-44c633386779",
     "showTitle": true,
     "title": "Otimiza diretório de gravação"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql('OPTIMIZE delta.`{}`'.format(OUTPUT_PATH))\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", False)\n",
    "spark.sql('VACUUM delta.`{}` RETAIN 0 HOURS'.format(OUTPUT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desmonta todos os volumes\n",
    "unmount('/mnt/')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "nb_abra_similaridade",
   "notebookOrigID": 1544361074205455,
   "widgets": {
    "aplicacao": {
     "currentValue": "abra",
     "nuid": "595376cf-1bcd-4a77-ab3a-7e1f96130a5c",
     "widgetInfo": {
      "defaultValue": "abra",
      "label": null,
      "name": "aplicacao",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "container": {
     "currentValue": "pouso",
     "nuid": "dd7a5acf-6c8c-4422-8aae-82ca14f9c436",
     "widgetInfo": {
      "defaultValue": "pouso",
      "label": null,
      "name": "container",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "data_anomalia_inicio": {
     "currentValue": "2016-06-27T11:17:34.705442Z",
     "nuid": "a1d87579-a925-4069-b98d-f5f92bfd0506",
     "widgetInfo": {
      "defaultValue": "2020-01-01",
      "label": null,
      "name": "data_anomalia_inicio",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "input_path_anomalia": {
     "currentValue": "abra/input/anomalia",
     "nuid": "30c9f00b-dcdf-4943-bb0b-126bc973614f",
     "widgetInfo": {
      "defaultValue": "abra/input/anomalia",
      "label": null,
      "name": "input_path_anomalia",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "input_path_resultado": {
     "currentValue": "abra/input/relacao_anomalia",
     "nuid": "30b1e09c-838b-4930-8a92-49287b0f373e",
     "widgetInfo": {
      "defaultValue": "abra/input/relacao_anomalia",
      "label": null,
      "name": "input_path_resultado",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "output_path": {
     "currentValue": "abra/output/relacao_anomalia",
     "nuid": "8b514868-26a4-44ea-9ad6-54ec7bf16a33",
     "widgetInfo": {
      "defaultValue": "abra/output/relacao_anomalia",
      "label": null,
      "name": "output_path",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "repartition": {
     "currentValue": "100",
     "nuid": "59dd0bd2-fc57-42af-88bb-096051c99aaa",
     "widgetInfo": {
      "defaultValue": "100",
      "label": null,
      "name": "repartition",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "storage_account": {
     "currentValue": "stdevptbdlkcsms",
     "nuid": "44d6abb8-e952-416f-8ea3-d9c420738825",
     "widgetInfo": {
      "defaultValue": "stdevptbdlkcsms",
      "label": null,
      "name": "storage_account",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "tabela_controle": {
     "currentValue": "controle_similaridade",
     "nuid": "bb44c597-284f-4198-97f2-3d4ac49e002a",
     "widgetInfo": {
      "defaultValue": "controle_similaridade",
      "label": null,
      "name": "tabela_controle",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
